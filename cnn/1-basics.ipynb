{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "- Brief Recap\n",
    "- Applications\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "Edge dectection: edged detection and other object detection are performed using filters on a given image.\n",
    "\n",
    "vertical edge detection filter:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Horizontal edge detection filter:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Usually, the sum of the elements in the filter is close to 0. This is because we want to preserve the brightness of the image.\n",
    "For vertical edge detection filter, the row-wise sum is 0. For horizontal edge detection filter, the column-wise sum is 0.\n",
    "\n",
    "The CNN is to learn filters automatically instead of manual designs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "**Filter**: \n",
    "\n",
    "- 3 x 3 x channels, for color images channels = 3, for grayscale images channels = 1\n",
    "- The values in the filter are not known, and are learned during training. \n",
    "\n",
    "**Stride**:\n",
    "- 1, 2, 3, etc. how many pixels to move the filter each time\n",
    "- striding:\n",
    "  - the original image is $(n, n)$, the filter is $(f, f)$, the stride is $s$\n",
    "  - after filter and stride convolution, the image size is $(\\frac{n-f}{s}+1, \\frac{n-f}{s}+1)$\n",
    "    - in case of non-integer, we can use floor \n",
    "    - the image size is then $(\\lfloor \\frac{n-f}{s} \\rfloor + 1, \\lfloor \\frac{n-f}{s} \\rfloor + 1)$\n",
    "\n",
    "**Padding**: add zeros around the image\n",
    "- after filtering and striding, the image size will be smaller, so we need padding to keep the image size the same if we dont want to shrink the image\n",
    "- add zeros around the image\n",
    "  - the original image is $(n, n)$, the filter is $(f, f)$, the padding is $p$, the stride is $s$\n",
    "    - padding $p$ means adding $p$ zeros around the image\n",
    "  - after padding $p$, the image is $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor$)\n",
    "- how much to pad?\n",
    "  - no padding: (n,n) -> (n-f+1, n-f+1)\n",
    "  - same convolution: padding so that the output size is same as the input size\n",
    "    - $n = n-f+2p+1$ -> $p = \\frac{f-1}{2}$\n",
    "\n",
    "**Convolutions over Volume**\n",
    "- RGB image -> 3D volume: $(n, n, n_c)$\n",
    "- filter -> 3D volume: $(f, f, n_c)$\n",
    "- padding $p$, stride $s$\n",
    "- the output is a 2D image: $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor$)\n",
    "  \n",
    "**Convolutional Layer**: A convolution layer is a set of filters. Each filter is a 3D tensor. The number of filters is the number of channels in the output image.\n",
    "- input: $(n, n, n_c)$\n",
    "- filter size: $(f, f, n_c)$, filter number: $n_f$, padding: $p$, stride: $s$\n",
    "  - filter output: $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, n_f)$\n",
    "- activation function: $a(z)$\n",
    "  - input: the filter output, and a bias term\n",
    "  - output; same size as the input\n",
    "- How many parameters?\n",
    "- How many parameters in one layer?\n",
    "  - filters: $n_f$ filters\n",
    "    - filter: $(f, f, n_c)$ -> $f^2n_c$\n",
    "    - bias: 1\n",
    "  - total: $n_f(f^2n_c+1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer \n",
    "**Pooling**: subsample the pixels will not change the object in the image\n",
    "- no weights to learn\n",
    "- max pooling: take the max value in the filter\n",
    "- average pooling: take the average value in the filter\n",
    "\n",
    "some notation:\n",
    "- because pooling usually has no weights, most literature doesn't regard pooling as a layer when counting the number of layers for a CNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical CNN architecture\n",
    "\n",
    "image -> convolutional layer -> pooling layer -> convolutional layer -> pooling layer -> flatten() -> fully connected layer -> output\n",
    "\n",
    "Why convolution?\n",
    "- sparsity of connections -> way less learnable parameters than fully connected layer \n",
    "  - fully connected layer: $n_x \\times n_y \\times n_c \\times n_f$\n",
    "  - cnn: ($f^2n_c+1) \\times n_f$\n",
    "- parameter sharing \n",
    "  - a feature dector that is useful in one part of the image is probably useful in another part of the image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "How backpropagation is implemented for CNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications\n",
    "\n",
    "**Alpha Go**\n",
    "\n",
    "CNN design:\n",
    "- input:\n",
    "  - the image size is 19 x 19\n",
    "  - channels = 48: humand designed channels to capture the features of the game\n",
    "- output: 19 x 19 -> where the next move should be\n",
    "\n",
    "Pooling is not used in Alpha Go, because pooling is like to remove columns and rows from the image, which is not good for board games. \n",
    "\n",
    "**Speech**\n",
    "Need consider the characteristics of speech when designning the CNN architecture.\n",
    "\n",
    "Reference:\n",
    "- https://dl.acm.org/doi/10.1109/TASLP.2014.2339736\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "Reference:\n",
    "- https://www.aclweb.org/anthology/S15-2079.pdf\n",
    "\n",
    "## Some criticism of CNN\n",
    "- CNN is not invariant to scale and rotation -> we need data augmentation\n",
    "- Spatial Transformer Layer can be used to solve this problem as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome Architectures\n",
    "\n",
    "### VGG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Net\n",
    "\n",
    "Residual net can be easy to learn an identity layer in a deep neural network, which can maintain the same performance as a shallower neural network. If got lucky, residual net can learn a better function than the shallower neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Net or GoogleNet\n",
    "\n",
    "Typical convolutional network shrink the height and width of the image, but increase the number of channels. \n",
    "1x1 convolution can be used to shrink the number of channels, but keep the height and width of the image.\n",
    "\n",
    "1x1 convolution is a simple convolutional layer with a filter size of 1x1, but add nonlinearity such as ReLu to it.\n",
    "\n",
    "Say we want to design a layer that can transform (28,28,192) to (28,28,32).\n",
    "For normal convolutional layer, if the filter size is 5x5, then the number of parameters is 32x5x5x192 = 150K, and the total multiplication is 150Kx28x28 = 1.2B.\n",
    "Using 1x1 convolution, the number of parameters is 32x1x1x192 = 6K, and the total multiplication is 6Kx28x28 = 4.7M.\n",
    "\n",
    "Inception module\n",
    "- input: previous layer: (28,28,192)\n",
    "- channel concatenate: stack the outputs of the following layers by channels\n",
    "  - 1x1 conv -> (28,28,64)\n",
    "  - 1x1 conv: (1,1,192,96) -> 3x3 conv -> (28, 28, 128) \n",
    "  - 1x1 conv: (1,1,192,16) -> 5x5 conv -> (28, 28, 32)\n",
    "  - 3x3 maxpool (padding for same, s=1) -> (28, 28, 192) -> 1x1 conv -> (28, 28, 32)\n",
    "- output: stacking lead to a layer that outputs (28, 28, 256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet\n",
    "\n",
    "used for mobile and embedded vision applications due to low computational cost\n",
    "\n",
    "key idea: normal vs depthwise-separable convolutions\n",
    "- normal convolution: (f, f, n_c) -> (n_h', n_w', n_c')\n",
    "- depthwise-separable convolution: (f, f, n_c) -> depthwise convolution: (n_h', n_w', 1) -> pointwise covolution: (1, 1, n_c)\n",
    "    - input: (n_h, n_w, n_c)\n",
    "    - depthwise: (f, f, n_c)\n",
    "      - output: (n_h', n_w', n_c)\n",
    "      - But we want n_c' instead of n_c.\n",
    "    - pointwise: (1, 1, n_c, n_c')\n",
    "      - output: (n_h', n_w', n_c')\n",
    "\n",
    "say we have an input of (6,6,3), we want to convolve it with a filter of (3,3,3,5) to output a (4,4,5)\n",
    "The computation cost in terms of multiplications:\n",
    "- normal convolution: # filter params * # filter operations * # of filters\n",
    "  - multiplications: (3x3x3) x (4x4) * 5 = 2160\n",
    "  - learnable parameters: (3x3x3)*5 = 135\n",
    "- depthwise convolution\n",
    "  - depthwise filter: (3,3,3)\n",
    "    - output: (4,4,3)\n",
    "    - multiplications: (3x3) * (4x4) * 3 = 432\n",
    "    - learnable parameters: (3x3)*3 = 27\n",
    "  - pointwise filter: (1,1,3,5)\n",
    "    - output: (4,4,5)\n",
    "    - multiplications: (1x1x3) * (4x4) * 5 = 240\n",
    "    - learnable parameters: (1x1x3) * 5 = 15\n",
    "  - in total:\n",
    "    - multiplications: 432 + 240 = 672  \n",
    "    - learnable parameters: 27 + 15 = 42\n",
    "- comparison:\n",
    "  - depthwise operations is faster, only 672/2160 = 0.3 of normal convolution operations\n",
    "  - smaller set of parameters: 42/135\n",
    "\n",
    "### MobileNet v1\n",
    "\n",
    "architecture:\n",
    "- input \n",
    "- module\n",
    "  - depthwise convolution\n",
    "  - pointwise convolution\n",
    "- output\n",
    "\n",
    "\n",
    "### Mobilenet v2\n",
    "two changes:\n",
    "- add a residual connection between input and output\n",
    "- add an expansion filter\n",
    "  - expansian filter\n",
    "  - depthwise filter\n",
    "  - projection filter (i.e., pointwise filter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet\n",
    "\n",
    "scale down or up a specific deep network for a particular device.\n",
    "\n",
    "three operations can scalue up/down of neural networks\n",
    "- resolution: resolution of input images\n",
    "- depth: depth of networks\n",
    "- width: make layers wider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Advice\n",
    "\n",
    "1. using open-source implementation\n",
    "2. transfer learning\n",
    "3. data augumentation\n",
    "   - mirroring\n",
    "   - shape/scale invariant\n",
    "     - random cropping\n",
    "     - rotation\n",
    "     - shearing\n",
    "     - local warping\n",
    "   - color shifting -> e.g., (R,G,B) + (20,-20,10) -> color invariant \n",
    "     - PCA color augumentation\n",
    "4. implementing distortions during training\n",
    "   - large images in hard disk\n",
    "   - multiple threads\n",
    "     - load image\n",
    "     - perform distortion\n",
    "     - formulate a mini-batch\n",
    "   - another CPU thread or GPU\n",
    "     - training\n",
    "5. tips for doing well on benchmarks/winning competitions\n",
    "   - ensembling\n",
    "     - train several (3-15) networks independently and average their outputs -> bagging, typically ~2% boosting\n",
    "       - rarely used in production\n",
    "   - multi-crop at test image\n",
    "     - run classifer on multiple version of test images and average results\n",
    "     - e.g., 10-crop\n",
    "     - slows down real-time inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
