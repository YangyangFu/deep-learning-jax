{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "- Brief Recap\n",
    "- Applications\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "Edge dectection: edged detection and other object detection are performed using filters on a given image.\n",
    "\n",
    "vertical edge detection filter:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Horizontal edge detection filter:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Usually, the sum of the elements in the filter is close to 0. This is because we want to preserve the brightness of the image.\n",
    "For vertical edge detection filter, the row-wise sum is 0. For horizontal edge detection filter, the column-wise sum is 0.\n",
    "\n",
    "The CNN is to learn filters automatically instead of manual designs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "**Filter**: \n",
    "\n",
    "- 3 x 3 x channels, for color images channels = 3, for grayscale images channels = 1\n",
    "- The values in the filter are not known, and are learned during training. \n",
    "\n",
    "**Stride**:\n",
    "- 1, 2, 3, etc. how many pixels to move the filter each time\n",
    "- striding:\n",
    "  - the original image is $(n, n)$, the filter is $(f, f)$, the stride is $s$\n",
    "  - after filter and stride convolution, the image size is $(\\frac{n-f}{s}+1, \\frac{n-f}{s}+1)$\n",
    "    - in case of non-integer, we can use floor \n",
    "    - the image size is then $(\\lfloor \\frac{n-f}{s} \\rfloor + 1, \\lfloor \\frac{n-f}{s} \\rfloor + 1)$\n",
    "\n",
    "**Padding**: add zeros around the image\n",
    "- after filtering and striding, the image size will be smaller, so we need padding to keep the image size the same if we dont want to shrink the image\n",
    "- add zeros around the image\n",
    "  - the original image is $(n, n)$, the filter is $(f, f)$, the padding is $p$, the stride is $s$\n",
    "    - padding $p$ means adding $p$ zeros around the image\n",
    "  - after padding $p$, the image is $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor$)\n",
    "- how much to pad?\n",
    "  - no padding: (n,n) -> (n-f+1, n-f+1)\n",
    "  - same convolution: padding so that the output size is same as the input size\n",
    "    - $n = n-f+2p+1$ -> $p = \\frac{f-1}{2}$\n",
    "\n",
    "**Convolutions over Volume**\n",
    "- RGB image -> 3D volume: $(n, n, n_c)$\n",
    "- filter -> 3D volume: $(f, f, n_c)$\n",
    "- padding $p$, stride $s$\n",
    "- the output is a 2D image: $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor$)\n",
    "  \n",
    "**Convolutional Layer**: A convolution layer is a set of filters. Each filter is a 3D tensor. The number of filters is the number of channels in the output image.\n",
    "- input: $(n, n, n_c)$\n",
    "- filter size: $(f, f, n_c)$, filter number: $n_f$, padding: $p$, stride: $s$\n",
    "  - filter output: $(\\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, \\lfloor \\frac{n-f+2p}{s} + 1 \\rfloor, n_f)$\n",
    "- activation function: $a(z)$\n",
    "  - input: the filter output, and a bias term\n",
    "  - output; same size as the input\n",
    "- How many parameters?\n",
    "- How many parameters in one layer?\n",
    "  - filters: $n_f$ filters\n",
    "    - filter: $(f, f, n_c)$ -> $f^2n_c$\n",
    "    - bias: 1\n",
    "  - total: $n_f(f^2n_c+1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer \n",
    "**Pooling**: subsample the pixels will not change the object in the image\n",
    "- no weights to learn\n",
    "- max pooling: take the max value in the filter\n",
    "- average pooling: take the average value in the filter\n",
    "\n",
    "some notation:\n",
    "- because pooling usually has no weights, most literature doesn't regard pooling as a layer when counting the number of layers for a CNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical CNN architecture\n",
    "\n",
    "image -> convolutional layer -> pooling layer -> convolutional layer -> pooling layer -> flatten() -> fully connected layer -> output\n",
    "\n",
    "Why convolution?\n",
    "- sparsity of connections -> way less learnable parameters than fully connected layer \n",
    "  - fully connected layer: $n_x \\times n_y \\times n_c \\times n_f$\n",
    "  - cnn: ($f^2n_c+1) \\times n_f$\n",
    "- parameter sharing \n",
    "  - a feature dector that is useful in one part of the image is probably useful in another part of the image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications\n",
    "\n",
    "**Alpha Go**\n",
    "\n",
    "CNN design:\n",
    "- input:\n",
    "  - the image size is 19 x 19\n",
    "  - channels = 48: humand designed channels to capture the features of the game\n",
    "- output: 19 x 19 -> where the next move should be\n",
    "\n",
    "Pooling is not used in Alpha Go, because pooling is like to remove columns and rows from the image, which is not good for board games. \n",
    "\n",
    "**Speech**\n",
    "Need consider the characteristics of speech when designning the CNN architecture.\n",
    "\n",
    "Reference:\n",
    "- https://dl.acm.org/doi/10.1109/TASLP.2014.2339736\n",
    "\n",
    "**Natural Language Processing**\n",
    "\n",
    "Reference:\n",
    "- https://www.aclweb.org/anthology/S15-2079.pdf\n",
    "\n",
    "## Some criticism of CNN\n",
    "- CNN is not invariant to scale and rotation -> we need data augmentation\n",
    "- Spatial Transformer Layer can be used to solve this problem as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome models\n",
    "\n",
    "### VGG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Net\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
