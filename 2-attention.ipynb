{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention\n",
    "\n",
    "Typical model input is a vector.\n",
    "What to do if we want a set of vectors as input, e.g., each word of different lengths in a sentence?\n",
    "- speech\n",
    "- graph is also a set of vectors, considering each node as a vector\n",
    "\n",
    "\n",
    "Output:\n",
    "- each vector has a label, therefore, the output is a set of vectors with labels\n",
    "  - e.g. sequence labelling, POS tagging: \"I saw a saw\" -> \"I/PRON saw/VERB a/DET saw/NOUN\"\n",
    "- the wholse sequence has a label\n",
    "  - e.g. sentiment analysis: \"This is good\" -> \"positive\" \n",
    "- model decides the number of labels itself \n",
    "  - e.g., sequence to sequence modeling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Labeling\n",
    "\n",
    "Sequences are in different lengths, what is the best way to represent them?\n",
    "\n",
    "Self-attention:\n",
    "- flow: sequence(words) -> self-attention -> FC layer -> self-attention -> FC layer -> output\n",
    "- self-attention layer: find the relevance of each word to each other word in the sequence\n",
    "  - relevance calculation: dot product of two vectors, or additively combine two vectors, etc.\n",
    "  - dot product\n",
    "    - query $q^1$: the word we are interested in\n",
    "    - key $k^1$: the word we are comparing to\n",
    "    - value: the word we are interested in\n",
    "\n",
    "Get the relevance of $a^1$ to all other vectors $a^\\{2,3,...,n\\}$:\n",
    "\n",
    "- self-attention layer\n",
    "  - specify parameters\n",
    "    - $q^1 = W^q a^1$\n",
    "    - $k^1 = W^k a^1$\n",
    "    - $k^2 = W^k a^2$\n",
    "    - ...\n",
    "    - $k^n = W^k a^n$\n",
    "  - get the relevance\n",
    "    - $\\alpha_{1,1} = q^1k^1$\n",
    "    - $\\alpha_{1,2} = q^1k^2$\n",
    "    - ...\n",
    "    - $\\alpha_{1,n} = q^1k^n$\n",
    "- softmax layer or others -> attention scores\n",
    "  - $\\alpha_{1,1} = \\frac{exp(\\alpha_{1,1})}{\\sum_{i=1}^n exp(\\alpha_{1,i})}$ \n",
    "- extract information based on attention scores\n",
    "  - $v^1 = W^va^1$\n",
    "  - $v^2 = W^va^2$\n",
    "  - ... \n",
    "  - $v^n = W^va^n$\n",
    "  - $b^1 = \\sum_{i=1}^n \\alpha_{1,i} v^i$ \n",
    "\n",
    "\n",
    "Matrix operations\n",
    "\n",
    "- $Q = W^q X$\n",
    "- $K = W^k X$\n",
    "- $V = W^v X$\n",
    "- self-attention matrix $A^{\\prime} \\rightarrow A = K^T Q$\n",
    "- $O = V A^{\\prime}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "Multi-head self-attention is to learn different types of relevance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In self-attention, the order of the sequence is not considered as the operation is the same for all vectors.\n",
    "Positional encoding is to add the position information to the vectors.\n",
    "- each position has a unique positional vector $p^i$, and add it to the original vector $a^i$ before Q,K,V operations.\n",
    "  - $a^i = a^i + p^i$\n",
    "- hand-crafted positional vector\n",
    "  - sinusoidal positional encoding\n",
    "  - $p^i_j = \\begin{cases} sin(\\frac{i}{10000^{2j/d}}) & \\text{if } j \\text{ is even} \\\\ cos(\\frac{i}{10000^{2j/d}}) & \\text{if } j \\text{ is odd} \\end{cases}$\n",
    "  - $i$: position\n",
    "  - $j$: dimension\n",
    "  - $d$: dimension of the vector\n",
    "- learnable positional vector\n",
    "- positional encoding is not necessary for RNNs as the order is considered in the operation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "- transformer\n",
    "- BERT\n",
    "- self-attention for speech\n",
    "  - truncated self-attention to avoid long sequences, big self-attention matrix\n",
    "- self-attention for images\n",
    "  - image is a set of vectors, each pixel is a vector of 3 for RGB images\n",
    "  - CNN is simplified self-attention, with fixed shape of the convolutional kernel\n",
    "    - ref: [on the relationship between self-attention and convolutional layers](https://arxiv.org/pdf/1911.03584.pdf)\n",
    "  - CNN model can be considered a subset of self-attention, thus CNN model is less complex. Therefore, CNN can have good results with less data, and self-attention needs more data to train.\n",
    "    - ref: [An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "\n",
    "- self-attention for time series\n",
    "  - RNN is now replaced by self-attention\n",
    "  \n",
    "  - self-attention vs RNN\n",
    "    - similar in terms of finding relationship between vectors\n",
    "    - dissimilar in terms of the operation\n",
    "      - RNN has to consider previous history by sequential operation. all histories are saved in memeory\n",
    "      - self-attention: parallel operation. no need to wait for the previous history to be calculated and forward.\n",
    "  - ref: \n",
    "    - [Transformers are RNNs: fast autoregressive transformers with linear attention](https://arxiv.org/abs/2006.16236)\n",
    "    - [Time Series Forecasting with Self-Attention Transformers](https://arxiv.org/pdf/1910.13051.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
